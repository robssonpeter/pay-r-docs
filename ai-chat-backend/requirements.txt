# ğŸ§  AI Chat Backend with LangChain, FAISS, and Ollama

This is a simple backend for a local document-based question-answering (QA) system using:

- [LangChain](https://www.langchain.com/)
- [FAISS](https://github.com/facebookresearch/faiss) for vector storage
- [Hugging Face Sentence Transformers](https://www.sbert.net/)
- [Ollama](https://ollama.com) for running LLMs locally (e.g. Mistral)
- [FastAPI](https://fastapi.tiangolo.com/) for serving the QA system via HTTP API

---

## ğŸš€ Features

- Load and index Markdown documents from a `docs/` folder
- Chunk documents for better context retrieval
- Generate and store embeddings using `sentence-transformers/all-MiniLM-L6-v2`
- Use a local Ollama LLM (e.g. `mistral`) for generating answers
- Query documents via an HTTP endpoint (`/ask`)

---

## ğŸ“ Project Structure

```bash
.
â”œâ”€â”€ ingest_docs.py       # One-time script to index markdown docs
â”œâ”€â”€ main.py              # FastAPI backend
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ .gitignore
â”œâ”€â”€ vectorstore/         # FAISS vector database (created after running ingest)
â””â”€â”€ docs/                # Markdown documents to be indexed
